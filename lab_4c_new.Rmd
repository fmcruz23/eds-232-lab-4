---
title: "Lab 4c. Deep Learning - iNaturalist"
author: Felicia Cruz
date: March 2, 2022
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
librarian::shelf(
  digest, dplyr, DT, glue, purrr, readr, stringr, tidyr, tensorflow, keras)

# path to folder containing species directories of images
dir_src  <- "/courses/EDS232/inaturalist-2021/train_mini"
dir_dest <- "~/eds-232-lab-4/inat"
dir.create(dir_dest, showWarnings = F)

# get list of directories, one per species (n = 10,000 species)
dirs_spp <- list.dirs(dir_src, recursive = F, full.names = T)
n_spp <- length(dirs_spp)

# set seed (for reproducible results) 
# just before sampling (otherwise get different results)
# based on your username (unique amongst class)
Sys.info()[["user"]] %>% 
  digest::digest2int() %>% 
  set.seed()
i10 <- sample(1:n_spp, 10)

# show the 10 indices sampled of the 10,000 possible 
i10
```

```{r}
# show the 10 species directory names
basename(dirs_spp)[i10]
```

```{r}
# show the first 2 species directory names
i2 <- i10[1:2]
basename(dirs_spp)[i2]
```

```{r}
# setup data frame with source (src) and destination (dest) paths to images
d <- tibble(
  set     = c(rep("spp2", 2), rep("spp10", 10)),
  dir_sp  = c(dirs_spp[i2], dirs_spp[i10]),
  tbl_img = map(dir_sp, function(dir_sp){
    tibble(
      src_img = list.files(dir_sp, full.names = T),
      subset  = c(rep("train", 30), rep("validation", 10), rep("test", 10))) })) %>% 
  unnest(tbl_img) %>% 
  mutate(
    sp       = basename(dir_sp),
    img      = basename(src_img),
    dest_img = glue("{dir_dest}/{set}/{subset}/{sp}/{img}"))

# show source and destination for first 10 rows of tibble
d %>% 
  select(src_img, dest_img)
```
```{r}
# iterate over rows, creating directory if needed and copying files 
d %>% 
  pwalk(function(src_img, dest_img, ...){
    dir.create(dirname(dest_img), recursive = T, showWarnings = F)
    file.copy(src_img, dest_img) })

# uncomment to show the entire tree of your destination directory
# system(glue("tree {dir_dest}"))
```

```{r}
# create base file paths
base_dir <- "/Users/feliciamcruz/eds-232-lab-4"
train_10_dir <- file.path(base_dir, "/inat/spp10/train")
train_2_dir <- file.path(base_dir, "/inat/spp2/train")
  
validation_10_dir <- file.path(base_dir, "/inat/spp10/validation")
validation_2_dir <- file.path(base_dir, "/inat/spp2/validation")
  
test_10_dir <- file.path(base_dir, "inat/spp10/test")
test_2_dir <- file.path(base_dir, "inat/spp2/test")

```

# Binary classifiction - neural net 

## pre-processing images 
```{r}
# All images will be rescaled by 1/255
test_datagen <- image_data_generator(rescale = 1/255)
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  # This is the target directory
  train_2_dir,
  # This is the data generator
  train_datagen,
  # All images will be resized to 150x150
  target_size = c(150, 150),
  batch_size = 30,
  # Since we use binary_crossentropy loss, we need binary labels
  class_mode = "binary") 

validation_generator <- flow_images_from_directory(
  validation_2_dir,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "binary")

batch <- generator_next(train_generator)
str(batch)
```
## build the network 
```{r}
model_1 <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units =  1, activation = "sigmoid")
```

```{r}
model_1 %>% compile(
  optimizer = "rmsprop",
  loss      = "binary_crossentropy",
  metrics   = c("accuracy"))
```

## fit the model
```{r}
history <- model_1 %>% fit(
    train_generator,
    steps_per_epoch = 2,
    epochs = 30,
    validation_data = validation_generator,
    validation_steps = 2)
```

```{r}
plot(history)
```

```{r}
history
```

## evaluate
```{r}
test_generator <- flow_images_from_directory(
  test_2_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "binary"
)

model_1 %>% evaluate(test_generator, steps = 2) 
```

**From the history plot above, we can see that loss decreases for the first few epics and levels off, and accuracy is generally increasing throughout the epics. On the test data, loss is 0.7 which is higher than the training and validation loss, and accuracy is 0.7 which is below the training accuracy but higher than the validation accuracy.**

# Binary classifiction - convolutional neural net

## build the network 
```{r}
# make the new model  

model_2 <- keras_model_sequential() %>% 
  layer_conv_2d(
    filters = 32, kernel_size = c(3, 3), activation = "relu",
    input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")  
  
model_2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc"))
```

## fit the model
```{r}
history_2 <- model_2 %>% fit(
    train_generator,
    steps_per_epoch = 2,
    epochs = 30,
    validation_data = validation_generator,
    validation_steps = 2)
```

```{r}
plot(history_2)
```

```{r}
history_2
```


## evaluate the model 
```{r}
test_generator_2 <- flow_images_from_directory(
  test_2_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "binary"
)

model_2 %>% evaluate(test_generator_2, steps = 2)
```

**From the history plot above, we can see that loss is generally decreasing and accuracy is increasing throughout the epics. On the test data, loss is 0.68 which is higher than both the test loss and the validation loss, and accuracy is 0.7 which is above both the training and validation accuracy.**

### Comparing standard neural network and convolutional neural network results

**Comparing the results from these two models above, I would choose the basic neural network because this model resulted in greater accuracy for both the training data and the validation data compared to the convolutional neural net.**

-----------------------------------

# Multi-class classifiction - neural net

## pre-processing images 
```{r}
# pre process the images from the 10 species using categorical class 

# All images will be rescaled by 1/255
test_datagen_10 <- image_data_generator(rescale = 1/255)
train_datagen_10 <- image_data_generator(rescale = 1/255)
validation_datagen_10 <- image_data_generator(rescale = 1/255)

train_generator_10 <- flow_images_from_directory(
  # This is the target directory
  train_10_dir,
  # This is the data generator
  train_datagen_10,
  # All images will be resized to 150x150
  target_size = c(150, 150),
  batch_size = 30,
  # change label to categorical 
  class_mode = "categorical") 

validation_generator_10 <- flow_images_from_directory(
  validation_10_dir,
  validation_datagen_10,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "categorical")

batch <- generator_next(train_generator_10)
str(batch)

```
## build the network 
```{r}
model_3 <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

# compile
model_3 %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

```

## fit the model 
```{r}
history_3 <- model_3 %>% fit(
  train_generator_10,
  steps_per_epoch = 1,
  epochs = 30,
  validation_data = validation_generator_10,
  validation_steps = 1)
```


## evaluate the model

```{r}
plot(history_3)
```


```{r}
history_3
```

```{r}
test_generator_3 <- flow_images_from_directory(
  test_10_dir,
  test_datagen_10,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "categorical"
)

model_3 %>% evaluate(test_generator_3, steps = 10)
```

**From the history plot above, we can see that loss is generally decreasing and accuracy is increasing throughout the epics. On the training data, loss is 5.3 which is lower than both the validation and the test loss, and accuracy is 0.2 which is equal to the validation accuracy and higher than the test accuracy.**

# multi-class classifiction - convolutional neural net 

## build the network 
```{r}
# make the new model  

model_4 <- keras_model_sequential() %>% 
  layer_conv_2d(
    filters = 32, kernel_size = c(3, 3), activation = "relu",
    input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")  
  
model_4 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("accuracy"))
```


## fit the model 
```{r}
history_4 <- model_4 %>% fit(
    train_generator_10,
    steps_per_epoch = 5,
    epochs = 30,
    validation_data = validation_generator_10,
    validation_steps = 5)
```


## evaluate the model 

```{r}
plot(history_4)
```

```{r}
history_4
```

```{r}
test_generator_4 <- flow_images_from_directory(
  test_10_dir,
  test_datagen_10,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "categorical"
)

model_4 %>% evaluate(test_generator_4, steps = 10)
```
**From the history plot above, we can see that loss is generally decreasing and accuracy is increasing throughout the epics. On the training data, loss is 1.5 which is lower than the test loss and the validation loss, and accuracy is 0.45 which is above both the training and validation accuracy.**

### Comparing standard neural network and convolutional neural network results

**Comparing the results from these two models above, I would choose the convolutional neural network because this model resulted in greater accuracy for both the training data and the validation data compared to the convolutional neural net.**
